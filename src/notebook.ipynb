{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z7JR-8kVLbB"
      },
      "source": [
        "# Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ-LgfNYVNhA",
        "outputId": "bf39d694-c74c-4eb0-cd8b-305b6e3c1c57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue May  6 14:23:26 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P0             29W /   70W |     864MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bolkf8GH6Xcr",
        "outputId": "16ddfe33-3274-4e31-9a27-581b3aa5a39d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ptflops in /usr/local/lib/python3.11/dist-packages (0.7.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from ptflops) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install ptflops tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qJyj-rST7ub"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import time\n",
        "from ptflops import get_model_complexity_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqP8k4z-_pCt"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFQmOXyO_k9Z"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5vN4wnlUHOh"
      },
      "outputs": [],
      "source": [
        "class SpatialReconstructionUnit(nn.Module):\n",
        "\n",
        "    def __init__(self, channels, gn_groups=32, threshold=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gn = nn.GroupNorm(num_groups=gn_groups, num_channels=channels)\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        x_norm = self.gn(x)\n",
        "        gamma = self.gn.weight.detach()\n",
        "        gamma = torch.clamp(gamma, min=1e-6)\n",
        "        w_gamma = gamma / (gamma.sum() + 1e-6)\n",
        "        mask = torch.sigmoid(w_gamma.view(1, C, 1, 1))\n",
        "        mask1 = (mask > self.threshold).float()\n",
        "        mask2 = (mask <= self.threshold).float()\n",
        "        x1 = x * mask1\n",
        "        x2 = x * mask2\n",
        "        x1_recon = x1 + x2\n",
        "        x2_recon = x2 + x1\n",
        "\n",
        "        return torch.cat([x1_recon, x2_recon], dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_Sw4BedUa0x"
      },
      "outputs": [],
      "source": [
        "class ChannelReconstructionUnit(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, split_ratio=0.5, squeeze_ratio=2, gwc_groups=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.alpha = split_ratio\n",
        "        C1 = int(self.alpha * in_channels)\n",
        "        C2 = in_channels - C1\n",
        "        C1s = C1 // squeeze_ratio\n",
        "        C2s = C2 // squeeze_ratio\n",
        "        self.squeeze1 = nn.Conv2d(C1, C1s, 1)\n",
        "        self.squeeze2 = nn.Conv2d(C2, C2s, 1)\n",
        "        self.gwc = nn.Conv2d(C1s, out_channels, 3, padding=1, groups=gwc_groups)\n",
        "        self.pw1 = nn.Conv2d(C1s, out_channels, 1)\n",
        "        self.pw2 = nn.Conv2d(C2s, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        C = x.size(1)\n",
        "        C1 = int(self.alpha * C)\n",
        "        x1, x2 = x[:, :C1], x[:, C1:]\n",
        "        xs1, xs2 = self.squeeze1(x1), self.squeeze2(x2)\n",
        "        y1 = self.gwc(xs1) + self.pw1(xs1)\n",
        "        y2 = self.pw2(xs2)\n",
        "        s1 = F.adaptive_avg_pool2d(y1, 1)\n",
        "        s2 = F.adaptive_avg_pool2d(y2, 1)\n",
        "        score = torch.cat([s1, s2], dim=1)\n",
        "        attn = F.softmax(score, dim=1)\n",
        "        beta1, beta2 = torch.split(attn, s1.size(1), dim=1)\n",
        "\n",
        "        return beta1 * y1 + beta2 * y2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8v5rRDBUdVp"
      },
      "outputs": [],
      "source": [
        "class SCConv(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels,\n",
        "                 gn_groups=32, threshold=0.5,\n",
        "                 split_ratio=0.5, squeeze_ratio=2, gwc_groups=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.sru = SpatialReconstructionUnit(in_channels, gn_groups, threshold)\n",
        "        self.cru = ChannelReconstructionUnit(in_channels*2, out_channels,\n",
        "                                             split_ratio, squeeze_ratio, gwc_groups)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.cru(self.sru(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynS-rYyEUmhN"
      },
      "outputs": [],
      "source": [
        "def conv1x1(in_planes, out_planes, stride=1): return nn.Conv2d(in_planes, out_planes, 1, stride=stride, bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcDIeTBZU9vr"
      },
      "outputs": [],
      "source": [
        "class BottleneckSC(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = conv1x1(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.scconv = SCConv(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = conv1x1(planes, planes*4)\n",
        "        self.bn3 = nn.BatchNorm2d(planes*4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.relu(self.bn2(self.scconv(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "\n",
        "        if self.downsample: identity = self.downsample(x)\n",
        "\n",
        "        return self.relu(out + identity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzYtCBMHZtcM"
      },
      "outputs": [],
      "source": [
        "class ResNetSC(nn.Module):\n",
        "\n",
        "    def __init__(self, layers, num_classes=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, 7, 2, 3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(3, 2, 1)\n",
        "        self.layer1 = self._make_layer(64, layers[0])\n",
        "        self.layer2 = self._make_layer(128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(512*4, num_classes)\n",
        "\n",
        "    def _make_layer(self, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "\n",
        "        if stride!=1 or self.inplanes!=planes*4:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes*4, stride),\n",
        "                nn.BatchNorm2d(planes*4)\n",
        "            )\n",
        "\n",
        "        layers = [BottleneckSC(self.inplanes, planes, stride, downsample)]\n",
        "        self.inplanes = planes*4\n",
        "\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BottleneckSC(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x); x = self.layer2(x)\n",
        "        x = self.layer3(x); x = self.layer4(x)\n",
        "        x = self.avgpool(x); x = torch.flatten(x,1)\n",
        "\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZJ5me42vfc8"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKgh4eIhZy3d"
      },
      "outputs": [],
      "source": [
        "# torch.autograd.set_detect_anomaly(True)\n",
        "def train_and_evaluate_10(use_scconv=False, epochs=60):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761))\n",
        "    ])\n",
        "\n",
        "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    testset = datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                                transform=transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761))\n",
        "                                ]))\n",
        "\n",
        "    trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "    testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    model = ResNetSC([3,4,6,3], num_classes=10).to(device) if use_scconv else models.resnet50(num_classes=10).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "    # scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,150], gamma=0.1)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train(); running_loss=0\n",
        "\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad(); outputs=model(inputs)\n",
        "            loss = criterion(outputs, labels); loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "            optimizer.step(); running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"> Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(trainloader):.4f}\")\n",
        "\n",
        "    model.eval(); correct=0; total=0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs); _, preds = outputs.max(1)\n",
        "            correct += preds.eq(labels).sum().item(); total += labels.size(0)\n",
        "\n",
        "    return 100. * correct / total, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewjkrMWevik5"
      },
      "outputs": [],
      "source": [
        "# torch.autograd.set_detect_anomaly(True)\n",
        "def train_and_evaluate_100(use_scconv=False, epochs=30):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761))\n",
        "    ])\n",
        "\n",
        "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "    testset = datasets.CIFAR100(root='./data', train=False, download=True,\n",
        "                                transform=transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761))\n",
        "                                ]))\n",
        "\n",
        "    trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "    testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    model = ResNetSC([3,4,6,3]).to(device) if use_scconv else models.resnet50(num_classes=100).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "    # scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,150], gamma=0.1)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train(); running_loss=0\n",
        "\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad(); outputs=model(inputs)\n",
        "            loss = criterion(outputs, labels); loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "            optimizer.step(); running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"> Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(trainloader):.4f}\")\n",
        "\n",
        "    model.eval(); correct=0; total=0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs); _, preds = outputs.max(1)\n",
        "            correct += preds.eq(labels).sum().item(); total += labels.size(0)\n",
        "\n",
        "    return 100. * correct / total, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUlYygue_zE_"
      },
      "source": [
        "# Driver Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RzOjcpjU__S",
        "outputId": "98f411c0-7c0d-4964-877a-1de9c537c792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Baseline ResNet50...\n",
            "> Epoch 1/60, Loss: 2.2999\n",
            "> Epoch 2/60, Loss: 2.0360\n",
            "> Epoch 3/60, Loss: 1.8608\n",
            "> Epoch 4/60, Loss: 1.6920\n",
            "> Epoch 5/60, Loss: 1.5395\n",
            "> Epoch 6/60, Loss: 1.4055\n",
            "> Epoch 7/60, Loss: 1.2741\n",
            "> Epoch 8/60, Loss: 1.1522\n",
            "> Epoch 9/60, Loss: 1.0653\n",
            "> Epoch 10/60, Loss: 0.9679\n",
            "> Epoch 11/60, Loss: 0.8967\n",
            "> Epoch 12/60, Loss: 0.8567\n",
            "> Epoch 13/60, Loss: 0.8251\n",
            "> Epoch 14/60, Loss: 0.7711\n",
            "> Epoch 15/60, Loss: 0.7287\n",
            "> Epoch 16/60, Loss: 0.7010\n",
            "> Epoch 17/60, Loss: 0.6825\n",
            "> Epoch 18/60, Loss: 0.6544\n",
            "> Epoch 19/60, Loss: 0.6299\n",
            "> Epoch 20/60, Loss: 0.6062\n",
            "> Epoch 21/60, Loss: 0.5042\n",
            "> Epoch 22/60, Loss: 0.4676\n",
            "> Epoch 23/60, Loss: 0.4545\n",
            "> Epoch 24/60, Loss: 0.4543\n",
            "> Epoch 25/60, Loss: 0.4377\n",
            "> Epoch 26/60, Loss: 0.4289\n",
            "> Epoch 27/60, Loss: 0.4198\n",
            "> Epoch 28/60, Loss: 0.4171\n",
            "> Epoch 29/60, Loss: 0.4096\n",
            "> Epoch 30/60, Loss: 0.4074\n",
            "> Epoch 31/60, Loss: 0.3963\n",
            "> Epoch 32/60, Loss: 0.3945\n",
            "> Epoch 33/60, Loss: 0.3833\n",
            "> Epoch 34/60, Loss: 0.3771\n",
            "> Epoch 35/60, Loss: 0.3723\n",
            "> Epoch 36/60, Loss: 0.3681\n",
            "> Epoch 37/60, Loss: 0.3594\n",
            "> Epoch 38/60, Loss: 0.3532\n",
            "> Epoch 39/60, Loss: 0.3523\n",
            "> Epoch 40/60, Loss: 0.3459\n",
            "> Epoch 41/60, Loss: 0.3280\n",
            "> Epoch 42/60, Loss: 0.3219\n",
            "> Epoch 43/60, Loss: 0.3216\n",
            "> Epoch 44/60, Loss: 0.3177\n",
            "> Epoch 45/60, Loss: 0.3175\n",
            "> Epoch 46/60, Loss: 0.3188\n",
            "> Epoch 47/60, Loss: 0.3141\n",
            "> Epoch 48/60, Loss: 0.3139\n",
            "> Epoch 49/60, Loss: 0.3129\n",
            "> Epoch 50/60, Loss: 0.3134\n",
            "> Epoch 51/60, Loss: 0.3106\n",
            "> Epoch 52/60, Loss: 0.3102\n",
            "> Epoch 53/60, Loss: 0.3089\n",
            "> Epoch 54/60, Loss: 0.3038\n",
            "> Epoch 55/60, Loss: 0.3067\n",
            "> Epoch 56/60, Loss: 0.3050\n",
            "> Epoch 57/60, Loss: 0.3018\n",
            "> Epoch 58/60, Loss: 0.3068\n",
            "> Epoch 59/60, Loss: 0.3010\n",
            "> Epoch 60/60, Loss: 0.3036\n",
            "\n",
            "Evaluating SCConv-ResNet50...\n",
            "> Epoch 1/60, Loss: 2.5703\n",
            "> Epoch 2/60, Loss: 2.1271\n",
            "> Epoch 3/60, Loss: 1.8434\n",
            "> Epoch 4/60, Loss: 1.6952\n",
            "> Epoch 5/60, Loss: 1.5174\n",
            "> Epoch 6/60, Loss: 1.3715\n",
            "> Epoch 7/60, Loss: 1.3166\n",
            "> Epoch 8/60, Loss: 1.2103\n",
            "> Epoch 9/60, Loss: 1.1464\n",
            "> Epoch 10/60, Loss: 1.0940\n",
            "> Epoch 11/60, Loss: 1.0251\n",
            "> Epoch 12/60, Loss: 0.9503\n",
            "> Epoch 13/60, Loss: 0.8813\n",
            "> Epoch 14/60, Loss: 0.8426\n",
            "> Epoch 15/60, Loss: 0.8255\n",
            "> Epoch 16/60, Loss: 0.8078\n",
            "> Epoch 17/60, Loss: 0.7656\n",
            "> Epoch 18/60, Loss: 0.7350\n",
            "> Epoch 19/60, Loss: 0.7032\n",
            "> Epoch 20/60, Loss: 0.6879\n",
            "> Epoch 21/60, Loss: 0.5705\n",
            "> Epoch 22/60, Loss: 0.5327\n",
            "> Epoch 23/60, Loss: 0.5203\n",
            "> Epoch 24/60, Loss: 0.5130\n",
            "> Epoch 25/60, Loss: 0.5001\n",
            "> Epoch 26/60, Loss: 0.4941\n",
            "> Epoch 27/60, Loss: 0.4867\n",
            "> Epoch 28/60, Loss: 0.4759\n",
            "> Epoch 29/60, Loss: 0.4741\n",
            "> Epoch 30/60, Loss: 0.4638\n",
            "> Epoch 31/60, Loss: 0.4618\n",
            "> Epoch 32/60, Loss: 0.4590\n",
            "> Epoch 33/60, Loss: 0.4427\n",
            "> Epoch 34/60, Loss: 0.4462\n",
            "> Epoch 35/60, Loss: 0.4445\n",
            "> Epoch 36/60, Loss: 0.4363\n",
            "> Epoch 37/60, Loss: 0.4341\n",
            "> Epoch 38/60, Loss: 0.4284\n",
            "> Epoch 39/60, Loss: 0.4201\n",
            "> Epoch 40/60, Loss: 0.4173\n",
            "> Epoch 41/60, Loss: 0.3970\n",
            "> Epoch 42/60, Loss: 0.3894\n",
            "> Epoch 43/60, Loss: 0.3878\n",
            "> Epoch 44/60, Loss: 0.3866\n",
            "> Epoch 45/60, Loss: 0.3810\n",
            "> Epoch 46/60, Loss: 0.3826\n",
            "> Epoch 47/60, Loss: 0.3809\n",
            "> Epoch 48/60, Loss: 0.3808\n",
            "> Epoch 49/60, Loss: 0.3787\n",
            "> Epoch 50/60, Loss: 0.3790\n",
            "> Epoch 51/60, Loss: 0.3765\n",
            "> Epoch 52/60, Loss: 0.3746\n",
            "> Epoch 53/60, Loss: 0.3794\n",
            "> Epoch 54/60, Loss: 0.3752\n",
            "> Epoch 55/60, Loss: 0.3760\n",
            "> Epoch 56/60, Loss: 0.3696\n",
            "> Epoch 57/60, Loss: 0.3717\n",
            "> Epoch 58/60, Loss: 0.3675\n",
            "> Epoch 59/60, Loss: 0.3683\n",
            "> Epoch 60/60, Loss: 0.3656\n",
            "\n",
            "Summary:\n",
            "\n",
            "╒══════════════════════╤════════════════╤═══════════════════════╤══════════════╤═════════════╕\n",
            "│ Model                │   Accuracy (%) │   Training Time (min) │   Params (M) │   FLOPs (G) │\n",
            "╞══════════════════════╪════════════════╪═══════════════════════╪══════════════╪═════════════╡\n",
            "│ ResNet-50 (Baseline) │          82.04 │                 29.47 │        23.53 │        0.08 │\n",
            "├──────────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ SCConv-ResNet-50     │          82.32 │                 42.62 │        17.58 │        0.06 │\n",
            "╘══════════════════════╧════════════════╧═══════════════════════╧══════════════╧═════════════╛\n",
            "\n",
            "Comparative Analysis:\n",
            "Accuracy Gain:      +0.28%\n",
            "Time Increase:      +789.21 sec (1.45× slower)\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    from tabulate import tabulate\n",
        "\n",
        "    print(\"Evaluating Baseline ResNet50...\")\n",
        "    start_base = time.time()\n",
        "    base_acc, base_model = train_and_evaluate_10(use_scconv=False)\n",
        "    end_base = time.time()\n",
        "\n",
        "    print(\"\\nEvaluating SCConv-ResNet50...\")\n",
        "    start_sc = time.time()\n",
        "    scconv_acc, scconv_model = train_and_evaluate_10(use_scconv=True)\n",
        "    end_sc = time.time()\n",
        "    acc_gain = scconv_acc - base_acc\n",
        "    time_diff = (end_sc - start_sc) - (end_base - start_base)\n",
        "    time_ratio = (end_sc - start_sc) / (end_base - start_base)\n",
        "    headers = [\"Model\", \"Accuracy (%)\", \"Training Time (min)\", \"Params (M)\", \"FLOPs (G)\"]\n",
        "\n",
        "    def get_stats(model):\n",
        "\n",
        "        with torch.cuda.device(0):\n",
        "            macs, params = get_model_complexity_info(model, (3, 32, 32), as_strings=False, print_per_layer_stat=False)\n",
        "\n",
        "            return round(params / 1e6, 2), round(macs / 1e9, 2)\n",
        "\n",
        "    base_params, base_flops = get_stats(base_model)\n",
        "    sc_params, sc_flops = get_stats(scconv_model)\n",
        "    rows = [\n",
        "        [\"ResNet-50 (Baseline)\", f\"{base_acc:.2f}\", f\"{(end_base-start_base)/60:.2f}\", f\"{base_params}\", f\"{base_flops}\"],\n",
        "        [\"SCConv-ResNet-50\", f\"{scconv_acc:.2f}\", f\"{(end_sc-start_sc)/60:.2f}\", f\"{sc_params}\", f\"{sc_flops}\"],\n",
        "    ]\n",
        "\n",
        "    print(\"\\nSummary:\\n\")\n",
        "    print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))\n",
        "    print(\"\\nComparative Analysis:\")\n",
        "    print(f\"Accuracy Gain:      {acc_gain:+.2f}%\")\n",
        "    print(f\"Time Increase:      {time_diff:+.2f} sec ({time_ratio:.2f}× slower)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcOKa1Xlx9l1",
        "outputId": "fb36c2b5-6f3d-4e46-ea40-66efe689224f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Baseline ResNet50...\n",
            "> Epoch 1/30, Loss: 4.6735\n",
            "> Epoch 2/30, Loss: 4.1758\n",
            "> Epoch 3/30, Loss: 3.7707\n",
            "> Epoch 4/30, Loss: 3.4680\n",
            "> Epoch 5/30, Loss: 3.2294\n",
            "> Epoch 6/30, Loss: 3.0319\n",
            "> Epoch 7/30, Loss: 2.8699\n",
            "> Epoch 8/30, Loss: 2.7406\n",
            "> Epoch 9/30, Loss: 2.6268\n",
            "> Epoch 10/30, Loss: 2.5031\n",
            "> Epoch 11/30, Loss: 2.4083\n",
            "> Epoch 12/30, Loss: 2.3169\n",
            "> Epoch 13/30, Loss: 2.2137\n",
            "> Epoch 14/30, Loss: 2.1361\n",
            "> Epoch 15/30, Loss: 2.0440\n",
            "> Epoch 16/30, Loss: 1.9689\n",
            "> Epoch 17/30, Loss: 1.8842\n",
            "> Epoch 18/30, Loss: 1.8095\n",
            "> Epoch 19/30, Loss: 1.7514\n",
            "> Epoch 20/30, Loss: 1.6743\n",
            "> Epoch 21/30, Loss: 1.3091\n",
            "> Epoch 22/30, Loss: 1.1632\n",
            "> Epoch 23/30, Loss: 1.1070\n",
            "> Epoch 24/30, Loss: 1.0480\n",
            "> Epoch 25/30, Loss: 1.0080\n",
            "> Epoch 26/30, Loss: 0.9674\n",
            "> Epoch 27/30, Loss: 0.9331\n",
            "> Epoch 28/30, Loss: 0.9021\n",
            "> Epoch 29/30, Loss: 0.8742\n",
            "> Epoch 30/30, Loss: 0.8463\n",
            "\n",
            "Evaluating SCConv-ResNet50...\n",
            "> Epoch 1/30, Loss: 4.8036\n",
            "> Epoch 2/30, Loss: 4.1170\n",
            "> Epoch 3/30, Loss: 3.8053\n",
            "> Epoch 4/30, Loss: 3.6144\n",
            "> Epoch 5/30, Loss: 3.4288\n",
            "> Epoch 6/30, Loss: 3.2801\n",
            "> Epoch 7/30, Loss: 3.1159\n",
            "> Epoch 8/30, Loss: 2.9676\n",
            "> Epoch 9/30, Loss: 2.8466\n",
            "> Epoch 10/30, Loss: 2.7305\n",
            "> Epoch 11/30, Loss: 2.6220\n",
            "> Epoch 12/30, Loss: 2.5069\n",
            "> Epoch 13/30, Loss: 2.4092\n",
            "> Epoch 14/30, Loss: 2.3302\n",
            "> Epoch 15/30, Loss: 2.2319\n",
            "> Epoch 16/30, Loss: 2.1635\n",
            "> Epoch 17/30, Loss: 2.0871\n",
            "> Epoch 18/30, Loss: 1.9965\n",
            "> Epoch 19/30, Loss: 1.9367\n",
            "> Epoch 20/30, Loss: 1.8701\n",
            "> Epoch 21/30, Loss: 1.5194\n",
            "> Epoch 22/30, Loss: 1.4154\n",
            "> Epoch 23/30, Loss: 1.3493\n",
            "> Epoch 24/30, Loss: 1.3089\n",
            "> Epoch 25/30, Loss: 1.2674\n",
            "> Epoch 26/30, Loss: 1.2364\n",
            "> Epoch 27/30, Loss: 1.1934\n",
            "> Epoch 28/30, Loss: 1.1601\n",
            "> Epoch 29/30, Loss: 1.1317\n",
            "> Epoch 30/30, Loss: 1.0933\n",
            "\n",
            "Summary:\n",
            "\n",
            "╒══════════════════════╤════════════════╤═══════════════════════╤══════════════╤═════════════╕\n",
            "│ Model                │   Accuracy (%) │   Training Time (min) │   Params (M) │   FLOPs (G) │\n",
            "╞══════════════════════╪════════════════╪═══════════════════════╪══════════════╪═════════════╡\n",
            "│ ResNet-50 (Baseline) │          49.49 │                 14.74 │        23.71 │        0.08 │\n",
            "├──────────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ SCConv-ResNet-50     │          51.04 │                 21.29 │        17.76 │        0.06 │\n",
            "╘══════════════════════╧════════════════╧═══════════════════════╧══════════════╧═════════════╛\n",
            "\n",
            "Comparative Analysis:\n",
            "Accuracy Gain:      +1.55%\n",
            "Time Increase:      +393.02 sec (1.44× slower)\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    from tabulate import tabulate\n",
        "\n",
        "    print(\"Evaluating Baseline ResNet50...\")\n",
        "    start_base = time.time()\n",
        "    base_acc, base_model = train_and_evaluate_100(use_scconv=False)\n",
        "    end_base = time.time()\n",
        "\n",
        "    print(\"\\nEvaluating SCConv-ResNet50...\")\n",
        "    start_sc = time.time()\n",
        "    scconv_acc, scconv_model = train_and_evaluate_100(use_scconv=True)\n",
        "    end_sc = time.time()\n",
        "    acc_gain = scconv_acc - base_acc\n",
        "    time_diff = (end_sc - start_sc) - (end_base - start_base)\n",
        "    time_ratio = (end_sc - start_sc) / (end_base - start_base)\n",
        "    headers = [\"Model\", \"Accuracy (%)\", \"Training Time (min)\", \"Params (M)\", \"FLOPs (G)\"]\n",
        "\n",
        "    def get_stats(model):\n",
        "\n",
        "        with torch.cuda.device(0):\n",
        "            macs, params = get_model_complexity_info(model, (3, 32, 32), as_strings=False, print_per_layer_stat=False)\n",
        "\n",
        "            return round(params / 1e6, 2), round(macs / 1e9, 2)\n",
        "\n",
        "    base_params, base_flops = get_stats(base_model)\n",
        "    sc_params, sc_flops = get_stats(scconv_model)\n",
        "    rows = [\n",
        "        [\"ResNet-50 (Baseline)\", f\"{base_acc:.2f}\", f\"{(end_base-start_base)/60:.2f}\", f\"{base_params}\", f\"{base_flops}\"],\n",
        "        [\"SCConv-ResNet-50\", f\"{scconv_acc:.2f}\", f\"{(end_sc-start_sc)/60:.2f}\", f\"{sc_params}\", f\"{sc_flops}\"],\n",
        "    ]\n",
        "\n",
        "    print(\"\\nSummary:\\n\")\n",
        "    print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))\n",
        "    print(\"\\nComparative Analysis:\")\n",
        "    print(f\"Accuracy Gain:      {acc_gain:+.2f}%\")\n",
        "    print(f\"Time Increase:      {time_diff:+.2f} sec ({time_ratio:.2f}× slower)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReXJE4W9vuAx"
      },
      "source": [
        "# Contribution-0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSA01CePvuAx"
      },
      "source": [
        "### Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5aMRuuPvuAx",
        "outputId": "c4f7417e-e56e-4d38-81f9-b349c51af27c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon May 12 09:56:42 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L40S                    Off | 00000000:34:00.0 Off |                    0 |\n",
            "| N/A   32C    P8              33W / 350W |      0MiB / 46068MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyMBYOf6vuAy",
        "outputId": "45192b10-1f17-4e0b-b87f-e3223462edd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ptflops in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.7.4)\n",
            "Requirement already satisfied: tabulate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.9.0)\n",
            "Requirement already satisfied: fvcore in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.5.post20221221)\n",
            "Requirement already satisfied: torch>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ptflops) (2.7.0+cu128)\n",
            "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore) (1.26.4)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore) (3.1.0)\n",
            "Requirement already satisfied: Pillow in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore) (11.2.1)\n",
            "Requirement already satisfied: iopath>=0.1.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore) (0.1.10)\n",
            "Requirement already satisfied: typing_extensions in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from iopath>=0.1.7->fvcore) (4.13.2)\n",
            "Requirement already satisfied: portalocker in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from iopath>=0.1.7->fvcore) (3.1.1)\n",
            "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (3.18.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (1.14.0)\n",
            "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (12.8.61)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (12.8.57)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (12.8.57)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (9.7.1.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (12.8.3.14)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (11.3.3.41)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (10.3.9.55)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (11.7.2.55)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (12.5.7.53)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (12.8.55)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (12.8.61)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (1.13.0.11)\n",
            "Requirement already satisfied: triton==3.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->ptflops) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from triton==3.3.0->torch>=2.0->ptflops) (78.1.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install ptflops tabulate fvcore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjuSqFaOvuAy"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.amp import autocast, GradScaler\n",
        "import torch\n",
        "import torch._dynamo\n",
        "import torch._dynamo as dynamo\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZqUslM7vuAy"
      },
      "outputs": [],
      "source": [
        "torch._dynamo.config.capture_scalar_outputs = True\n",
        "\n",
        "# Enabling cuDNN autotuner for optimized kernels\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Enabling TF32 on Ampere GPUs for faster FP32 matmuls\n",
        "if hasattr(torch.backends.cuda, 'matmul_precision'):\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjk00OVUvuAy"
      },
      "source": [
        "### Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIjdr-R1vuAy"
      },
      "source": [
        "##### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uZlf9K9vuTX"
      },
      "outputs": [],
      "source": [
        "def lean_scconv_kernel(x, weight_dw, weight_pw):\n",
        "    # depthwise + pointwise fused via torch.compile\n",
        "    x = F.conv2d(x, weight_dw, padding=1, groups=weight_dw.shape[0])\n",
        "\n",
        "    return F.conv2d(x, weight_pw)\n",
        "\n",
        "class SCConv(nn.Module):\n",
        "\n",
        "    def __init__(self, channels):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.weight_dw = nn.Parameter(torch.empty(channels, 1, 3, 3))\n",
        "        self.weight_pw = nn.Parameter(torch.empty(channels, channels, 1, 1))\n",
        "        nn.init.kaiming_normal_(self.weight_dw, mode='fan_out', nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.weight_pw, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ensure channels-last\n",
        "        x = x.contiguous(memory_format=torch.channels_last)\n",
        "        out = lean_scconv_kernel(x, self.weight_dw, self.weight_pw)\n",
        "\n",
        "        return out.contiguous(memory_format=torch.contiguous_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioA8-vQ9vuTX"
      },
      "outputs": [],
      "source": [
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, 1, stride=stride, bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_BphqE0vuTX"
      },
      "outputs": [],
      "source": [
        "class BottleneckSC(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, use_scconv=True):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, 1, stride=stride, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(planes)\n",
        "        self.relu  = nn.ReLU(inplace=True)\n",
        "\n",
        "        # use SCConv only in later stages, based on channel size\n",
        "        self.sc    = SCConv(planes) if use_scconv else nn.Conv2d(planes, planes, 3, padding=1, groups=planes, bias=False)\n",
        "        self.bn2   = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
        "        self.bn3   = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.sc(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "\n",
        "        return self.relu(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9bvT2WuvuTY"
      },
      "outputs": [],
      "source": [
        "class ResNetSC(nn.Module):\n",
        "\n",
        "    def __init__(self, layers, num_classes=10, use_scconv=True):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, 7, 2, 3, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(64)\n",
        "        self.relu  = nn.ReLU(inplace=True)\n",
        "        self.maxp  = nn.MaxPool2d(3, 2, 1)\n",
        "\n",
        "        # selectively disable SCConv in first block\n",
        "        self.layer1 = self._make_layer(64, layers[0], use_scconv=False)\n",
        "        self.layer2 = self._make_layer(128, layers[1], use_scconv=True, stride=2)\n",
        "        self.layer3 = self._make_layer(256, layers[2], use_scconv=True, stride=2)\n",
        "        self.layer4 = self._make_layer(512, layers[3], use_scconv=True, stride=2)\n",
        "        self.avgp = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc   = nn.Linear(512 * BottleneckSC.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, planes, blocks, use_scconv, stride=1):\n",
        "        downsample = None\n",
        "\n",
        "        if stride != 1 or self.inplanes != planes * BottleneckSC.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * BottleneckSC.expansion, 1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * BottleneckSC.expansion)\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(BottleneckSC(self.inplanes, planes, stride, downsample, use_scconv))\n",
        "        self.inplanes = planes * BottleneckSC.expansion\n",
        "\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BottleneckSC(self.inplanes, planes, use_scconv=use_scconv))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # enforce channels-last at input\n",
        "        x = x.to(memory_format=torch.channels_last)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxp(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgp(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5yaztXYvuA1"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF9wn7O4vuTY"
      },
      "outputs": [],
      "source": [
        "def get_data_loaders_10():\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "\n",
        "    trainset = datasets.CIFAR10('./data', True, transform_train, download=True)\n",
        "    testset  = datasets.CIFAR10('./data', False, transform_test, download=True)\n",
        "\n",
        "    trainloader = DataLoader(\n",
        "        trainset, 128, shuffle=True,\n",
        "        num_workers=8, pin_memory=True,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    testloader  = DataLoader(\n",
        "        testset,  100, shuffle=False,\n",
        "        num_workers=8, pin_memory=True,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "def get_data_loaders_100():\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "\n",
        "    trainset = datasets.CIFAR100('./data', True, transform_train, download=True)\n",
        "    testset  = datasets.CIFAR100('./data', False, transform_test, download=True)\n",
        "\n",
        "    trainloader = DataLoader(\n",
        "        trainset, 128, shuffle=True,\n",
        "        num_workers=8, pin_memory=True,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    testloader  = DataLoader(\n",
        "        testset,  100, shuffle=False,\n",
        "        num_workers=8, pin_memory=True,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    return trainloader, testloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc40YF1kvuTY"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(dataset_cifar_100=False, use_scconv=False, epochs=60):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    if dataset_cifar_100:\n",
        "        trainloader, testloader = get_data_loaders_100()\n",
        "\n",
        "        # initialize and compile model\n",
        "        if use_scconv:\n",
        "            model = ResNetSC([3,4,6,3], num_classes=100, use_scconv=True)\n",
        "        else:\n",
        "            model = models.resnet50(num_classes=100)\n",
        "\n",
        "        epochs = 30\n",
        "    else:\n",
        "        trainloader, testloader = get_data_loaders_10()\n",
        "\n",
        "        # initialize and compile model\n",
        "        if use_scconv:\n",
        "            model = ResNetSC([3,4,6,3], num_classes=10, use_scconv=True)\n",
        "        else:\n",
        "            model = models.resnet50(num_classes=10)\n",
        "\n",
        "    model = model.to(device)\n",
        "    # model = model.to(memory_format=torch.channels_last)\n",
        "\n",
        "    model = torch.compile(model)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "    # scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    #     optimizer, max_lr=0.1,\n",
        "    #     steps_per_epoch=len(trainloader), epochs=epochs, gamma=0.1\n",
        "    # )\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        loss_accum = 0.0\n",
        "\n",
        "        for imgs, lbls in trainloader:\n",
        "            imgs = imgs.to(device=device, memory_format=torch.channels_last, non_blocking=True)\n",
        "            lbls = lbls.to(device=device, non_blocking=True)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with autocast(device_type='cuda'):\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, lbls)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            loss_accum += loss.item()\n",
        "\n",
        "        print(f\"> Epoch {epoch+1}/{epochs}, Loss: {loss_accum/len(trainloader):.4f}\")\n",
        "\n",
        "    # evaluation\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for imgs, lbls in testloader:\n",
        "            imgs = imgs.to(device, non_blocking=True)\n",
        "            lbls = lbls.to(device, non_blocking=True)\n",
        "\n",
        "            with autocast(device_type='cuda'):\n",
        "                preds = model(imgs).argmax(dim=1)\n",
        "\n",
        "            correct += (preds == lbls).sum().item()\n",
        "            total += lbls.size(0)\n",
        "\n",
        "    return 100. * correct / total, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNYY097lvuA2"
      },
      "source": [
        "### Driver Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FroQnCuvuA2",
        "outputId": "fc7ff405-3243-4c3f-b751-8a34603445de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Baseline ResNet50...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Epoch 1/60, Loss: 2.5179\n",
            "> Epoch 2/60, Loss: 2.3780\n",
            "> Epoch 3/60, Loss: 2.3698\n",
            "> Epoch 4/60, Loss: 2.3769\n",
            "> Epoch 5/60, Loss: 2.3660\n",
            "> Epoch 6/60, Loss: 2.3681\n",
            "> Epoch 7/60, Loss: 2.3751\n",
            "> Epoch 8/60, Loss: 2.3744\n",
            "> Epoch 9/60, Loss: 2.3699\n",
            "> Epoch 10/60, Loss: 2.3800\n",
            "> Epoch 11/60, Loss: 2.3746\n",
            "> Epoch 12/60, Loss: 2.3791\n",
            "> Epoch 13/60, Loss: 2.3681\n",
            "> Epoch 14/60, Loss: 2.3737\n",
            "> Epoch 15/60, Loss: 2.3782\n",
            "> Epoch 16/60, Loss: 2.3748\n",
            "> Epoch 17/60, Loss: 2.3785\n",
            "> Epoch 18/60, Loss: 2.3804\n",
            "> Epoch 19/60, Loss: 2.3698\n",
            "> Epoch 20/60, Loss: 2.3785\n",
            "> Epoch 21/60, Loss: 2.3760\n",
            "> Epoch 22/60, Loss: 2.3765\n",
            "> Epoch 23/60, Loss: 2.3758\n",
            "> Epoch 24/60, Loss: 2.3799\n",
            "> Epoch 25/60, Loss: 2.3680\n",
            "> Epoch 26/60, Loss: 2.3740\n",
            "> Epoch 27/60, Loss: 2.3659\n",
            "> Epoch 28/60, Loss: 2.3800\n",
            "> Epoch 29/60, Loss: 2.3735\n",
            "> Epoch 30/60, Loss: 2.3779\n",
            "> Epoch 31/60, Loss: 2.3793\n",
            "> Epoch 32/60, Loss: 2.3803\n",
            "> Epoch 33/60, Loss: 2.3765\n",
            "> Epoch 34/60, Loss: 2.3777\n",
            "> Epoch 35/60, Loss: 2.3705\n",
            "> Epoch 36/60, Loss: 2.3721\n",
            "> Epoch 37/60, Loss: 2.3785\n",
            "> Epoch 38/60, Loss: 2.3808\n",
            "> Epoch 39/60, Loss: 2.3776\n",
            "> Epoch 40/60, Loss: 2.3772\n",
            "> Epoch 41/60, Loss: 2.3742\n",
            "> Epoch 42/60, Loss: 2.3763\n",
            "> Epoch 43/60, Loss: 2.3751\n",
            "> Epoch 44/60, Loss: 2.3751\n",
            "> Epoch 45/60, Loss: 2.3801\n",
            "> Epoch 46/60, Loss: 2.3728\n",
            "> Epoch 47/60, Loss: 2.3718\n",
            "> Epoch 48/60, Loss: 2.3754\n",
            "> Epoch 49/60, Loss: 2.3812\n",
            "> Epoch 50/60, Loss: 2.3748\n",
            "> Epoch 51/60, Loss: 2.3771\n",
            "> Epoch 52/60, Loss: 2.3749\n",
            "> Epoch 53/60, Loss: 2.3794\n",
            "> Epoch 54/60, Loss: 2.3712\n",
            "> Epoch 55/60, Loss: 2.3735\n",
            "> Epoch 56/60, Loss: 2.3791\n",
            "> Epoch 57/60, Loss: 2.3693\n",
            "> Epoch 58/60, Loss: 2.3801\n",
            "> Epoch 59/60, Loss: 2.3632\n",
            "> Epoch 60/60, Loss: 2.3776\n",
            "\n",
            "Evaluating SCConv-ResNet50...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Epoch 1/60, Loss: 2.4098\n",
            "> Epoch 2/60, Loss: 2.3188\n",
            "> Epoch 3/60, Loss: 2.3120\n",
            "> Epoch 4/60, Loss: 2.3077\n",
            "> Epoch 5/60, Loss: 2.3124\n",
            "> Epoch 6/60, Loss: 2.2995\n",
            "> Epoch 7/60, Loss: 2.2961\n",
            "> Epoch 8/60, Loss: 2.3126\n",
            "> Epoch 9/60, Loss: 2.3217\n",
            "> Epoch 10/60, Loss: 2.3117\n",
            "> Epoch 11/60, Loss: 2.3110\n",
            "> Epoch 12/60, Loss: 2.2995\n",
            "> Epoch 13/60, Loss: 2.3014\n",
            "> Epoch 14/60, Loss: 2.3135\n",
            "> Epoch 15/60, Loss: 2.2900\n",
            "> Epoch 16/60, Loss: 2.3030\n",
            "> Epoch 17/60, Loss: 2.3023\n",
            "> Epoch 18/60, Loss: 2.3220\n",
            "> Epoch 19/60, Loss: 2.3051\n",
            "> Epoch 20/60, Loss: 2.3066\n",
            "> Epoch 21/60, Loss: 2.3188\n",
            "> Epoch 22/60, Loss: 2.3160\n",
            "> Epoch 23/60, Loss: 2.3124\n",
            "> Epoch 24/60, Loss: 2.3198\n",
            "> Epoch 25/60, Loss: 2.3084\n",
            "> Epoch 26/60, Loss: 2.3094\n",
            "> Epoch 27/60, Loss: 2.3060\n",
            "> Epoch 28/60, Loss: 2.3095\n",
            "> Epoch 29/60, Loss: 2.3054\n",
            "> Epoch 30/60, Loss: 2.3062\n",
            "> Epoch 31/60, Loss: 2.3114\n",
            "> Epoch 32/60, Loss: 2.3061\n",
            "> Epoch 33/60, Loss: 2.2920\n",
            "> Epoch 34/60, Loss: 2.3140\n",
            "> Epoch 35/60, Loss: 2.2983\n",
            "> Epoch 36/60, Loss: 2.3078\n",
            "> Epoch 37/60, Loss: 2.3146\n",
            "> Epoch 38/60, Loss: 2.3028\n",
            "> Epoch 39/60, Loss: 2.3185\n",
            "> Epoch 40/60, Loss: 2.3013\n",
            "> Epoch 41/60, Loss: 2.3145\n",
            "> Epoch 42/60, Loss: 2.3031\n",
            "> Epoch 43/60, Loss: 2.2978\n",
            "> Epoch 44/60, Loss: 2.3094\n",
            "> Epoch 45/60, Loss: 2.3203\n",
            "> Epoch 46/60, Loss: 2.3033\n",
            "> Epoch 47/60, Loss: 2.3003\n",
            "> Epoch 48/60, Loss: 2.3012\n",
            "> Epoch 49/60, Loss: 2.3193\n",
            "> Epoch 50/60, Loss: 2.3058\n",
            "> Epoch 51/60, Loss: 2.2951\n",
            "> Epoch 52/60, Loss: 2.3093\n",
            "> Epoch 53/60, Loss: 2.3124\n",
            "> Epoch 54/60, Loss: 2.3045\n",
            "> Epoch 55/60, Loss: 2.3118\n",
            "> Epoch 56/60, Loss: 2.3193\n",
            "> Epoch 57/60, Loss: 2.3228\n",
            "> Epoch 58/60, Loss: 2.3181\n",
            "> Epoch 59/60, Loss: 2.3045\n",
            "> Epoch 60/60, Loss: 2.2983\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add_ encountered 16 time(s)\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add_ encountered 16 time(s)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summary:\n",
            "\n",
            "╒══════════════════════╤════════════════╤═══════════════════════╤══════════════╤═════════════╕\n",
            "│ Model                │   Accuracy (%) │   Training Time (min) │   Params (M) │   FLOPs (G) │\n",
            "╞══════════════════════╪════════════════╪═══════════════════════╪══════════════╪═════════════╡\n",
            "│ ResNet-50 (Baseline) │          17.52 │                  9.01 │     23528522 │        0.08 │\n",
            "├──────────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ SCConv-ResNet-50     │          20.84 │                  8.85 │     13490442 │        0.05 │\n",
            "╘══════════════════════╧════════════════╧═══════════════════════╧══════════════╧═════════════╛\n",
            "\n",
            "Comparative Analysis:\n",
            "Accuracy Gain:      +3.32%\n",
            "Time Increase:      -9.57 sec (0.98× slower)\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    from tabulate import tabulate\n",
        "\n",
        "    print(\"Evaluating Baseline ResNet50...\")\n",
        "    start_base = time.time()\n",
        "    base_acc, base_model = train_and_evaluate(dataset_cifar_100=False, use_scconv=False)\n",
        "    end_base = time.time()\n",
        "\n",
        "    print(\"\\nEvaluating SCConv-ResNet50...\")\n",
        "    start_sc = time.time()\n",
        "    scconv_acc, scconv_model = train_and_evaluate(dataset_cifar_100=False, use_scconv=True)\n",
        "    end_sc = time.time()\n",
        "    acc_gain = scconv_acc - base_acc\n",
        "    time_diff = (end_sc - start_sc) - (end_base - start_base)\n",
        "    time_ratio = (end_sc - start_sc) / (end_base - start_base)\n",
        "    headers = [\"Model\", \"Accuracy (%)\", \"Training Time (min)\", \"Params (M)\", \"FLOPs (G)\"]\n",
        "\n",
        "    print()\n",
        "\n",
        "    from fvcore.nn import FlopCountAnalysis, parameter_count\n",
        "\n",
        "    # def get_stats_fvcore(model, input_size=(1, 3, 32, 32)):\n",
        "    #     model = model.eval().cpu()\n",
        "    #     inputs = torch.randn(*input_size)\n",
        "    #     flops = FlopCountAnalysis(model, inputs)\n",
        "    #     params = parameter_count(model)\n",
        "\n",
        "    #     return round(params[\"\"], 2), round(flops.total() / 1e9, 2)\n",
        "\n",
        "    # base_params, base_flops = get_stats_fvcore(base_model)\n",
        "    # sc_params, sc_flops = get_stats_fvcore(scconv_model)\n",
        "\n",
        "    def get_stats_fvcore_from_scratch(use_scconv=False, input_size=(1, 3, 32, 32)):\n",
        "\n",
        "        if use_scconv:\n",
        "            model = ResNetSC([3, 4, 6, 3], num_classes=10)\n",
        "        else:\n",
        "            model = models.resnet50(num_classes=10)\n",
        "\n",
        "        model = model.eval().cpu()\n",
        "        inputs = torch.randn(*input_size)\n",
        "        flops = FlopCountAnalysis(model, inputs)\n",
        "        params = parameter_count(model)\n",
        "\n",
        "        return round(params[\"\"], 2), round(flops.total() / 1e9, 2)\n",
        "\n",
        "    base_params, base_flops = get_stats_fvcore_from_scratch(use_scconv=False)\n",
        "    sc_params, sc_flops = get_stats_fvcore_from_scratch(use_scconv=True)\n",
        "\n",
        "    rows = [\n",
        "        [\"ResNet-50 (Baseline)\", f\"{base_acc:.2f}\", f\"{(end_base-start_base)/60:.2f}\", f\"{base_params}\", f\"{base_flops}\"],\n",
        "        [\"SCConv-ResNet-50\", f\"{scconv_acc:.2f}\", f\"{(end_sc-start_sc)/60:.2f}\", f\"{sc_params}\", f\"{sc_flops}\"],\n",
        "    ]\n",
        "\n",
        "    print(\"\\nSummary:\\n\")\n",
        "    print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))\n",
        "    print(\"\\nComparative Analysis:\")\n",
        "    print(f\"Accuracy Gain:      {acc_gain:+.2f}%\")\n",
        "    print(f\"Time Increase:      {time_diff:+.2f} sec ({time_ratio:.2f}× slower)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMBSINaMvuA2",
        "outputId": "734c3c62-0db9-43ae-88fe-bc225598410f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Baseline ResNet50...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Epoch 1/30, Loss: 4.7570\n",
            "> Epoch 2/30, Loss: 4.7140\n",
            "> Epoch 3/30, Loss: 4.7128\n",
            "> Epoch 4/30, Loss: 4.7124\n",
            "> Epoch 5/30, Loss: 4.7181\n",
            "> Epoch 6/30, Loss: 4.7098\n",
            "> Epoch 7/30, Loss: 4.7175\n",
            "> Epoch 8/30, Loss: 4.7104\n",
            "> Epoch 9/30, Loss: 4.7101\n",
            "> Epoch 10/30, Loss: 4.7127\n",
            "> Epoch 11/30, Loss: 4.7144\n",
            "> Epoch 12/30, Loss: 4.7143\n",
            "> Epoch 13/30, Loss: 4.7170\n",
            "> Epoch 14/30, Loss: 4.7132\n",
            "> Epoch 15/30, Loss: 4.7120\n",
            "> Epoch 16/30, Loss: 4.7132\n",
            "> Epoch 17/30, Loss: 4.7151\n",
            "> Epoch 18/30, Loss: 4.7103\n",
            "> Epoch 19/30, Loss: 4.7101\n",
            "> Epoch 20/30, Loss: 4.7194\n",
            "> Epoch 21/30, Loss: 4.7091\n",
            "> Epoch 22/30, Loss: 4.7086\n",
            "> Epoch 23/30, Loss: 4.7186\n",
            "> Epoch 24/30, Loss: 4.7122\n",
            "> Epoch 25/30, Loss: 4.7119\n",
            "> Epoch 26/30, Loss: 4.7142\n",
            "> Epoch 27/30, Loss: 4.7119\n",
            "> Epoch 28/30, Loss: 4.7152\n",
            "> Epoch 29/30, Loss: 4.7141\n",
            "> Epoch 30/30, Loss: 4.7131\n",
            "\n",
            "Evaluating SCConv-ResNet50...\n",
            "> Epoch 1/30, Loss: 4.8543\n",
            "> Epoch 2/30, Loss: 4.8073\n",
            "> Epoch 3/30, Loss: 4.8005\n",
            "> Epoch 4/30, Loss: 4.8073\n",
            "> Epoch 5/30, Loss: 4.8044\n",
            "> Epoch 6/30, Loss: 4.8011\n",
            "> Epoch 7/30, Loss: 4.8082\n",
            "> Epoch 8/30, Loss: 4.8059\n",
            "> Epoch 9/30, Loss: 4.8054\n",
            "> Epoch 10/30, Loss: 4.8094\n",
            "> Epoch 11/30, Loss: 4.8132\n",
            "> Epoch 12/30, Loss: 4.8099\n",
            "> Epoch 13/30, Loss: 4.8036\n",
            "> Epoch 14/30, Loss: 4.7948\n",
            "> Epoch 15/30, Loss: 4.8095\n",
            "> Epoch 16/30, Loss: 4.8064\n",
            "> Epoch 17/30, Loss: 4.8020\n",
            "> Epoch 18/30, Loss: 4.7997\n",
            "> Epoch 19/30, Loss: 4.8008\n",
            "> Epoch 20/30, Loss: 4.8151\n",
            "> Epoch 21/30, Loss: 4.8031\n",
            "> Epoch 22/30, Loss: 4.8103\n",
            "> Epoch 23/30, Loss: 4.8138\n",
            "> Epoch 24/30, Loss: 4.8098\n",
            "> Epoch 25/30, Loss: 4.8164\n",
            "> Epoch 26/30, Loss: 4.8141\n",
            "> Epoch 27/30, Loss: 4.8089\n",
            "> Epoch 28/30, Loss: 4.8116\n",
            "> Epoch 29/30, Loss: 4.8110\n",
            "> Epoch 30/30, Loss: 4.8136\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add_ encountered 16 time(s)\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add_ encountered 16 time(s)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summary:\n",
            "\n",
            "╒══════════════════════╤════════════════╤═══════════════════════╤══════════════╤═════════════╕\n",
            "│ Model                │   Accuracy (%) │   Training Time (min) │   Params (M) │   FLOPs (G) │\n",
            "╞══════════════════════╪════════════════╪═══════════════════════╪══════════════╪═════════════╡\n",
            "│ ResNet-50 (Baseline) │           2.26 │                  4.27 │     23528522 │        0.08 │\n",
            "├──────────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ SCConv-ResNet-50     │           2.95 │                  4.19 │     13674852 │        0.05 │\n",
            "╘══════════════════════╧════════════════╧═══════════════════════╧══════════════╧═════════════╛\n",
            "\n",
            "Comparative Analysis:\n",
            "Accuracy Gain:      +0.69%\n",
            "Time Increase:      -4.95 sec (0.98× slower)\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    from tabulate import tabulate\n",
        "\n",
        "    print(\"Evaluating Baseline ResNet50...\")\n",
        "    start_base = time.time()\n",
        "    base_acc, base_model = train_and_evaluate(dataset_cifar_100=True, use_scconv=False)\n",
        "    end_base = time.time()\n",
        "\n",
        "    print(\"\\nEvaluating SCConv-ResNet50...\")\n",
        "    start_sc = time.time()\n",
        "    scconv_acc, scconv_model = train_and_evaluate(dataset_cifar_100=True, use_scconv=True)\n",
        "    end_sc = time.time()\n",
        "    acc_gain = scconv_acc - base_acc\n",
        "    time_diff = (end_sc - start_sc) - (end_base - start_base)\n",
        "    time_ratio = (end_sc - start_sc) / (end_base - start_base)\n",
        "    headers = [\"Model\", \"Accuracy (%)\", \"Training Time (min)\", \"Params (M)\", \"FLOPs (G)\"]\n",
        "\n",
        "    print()\n",
        "\n",
        "    from fvcore.nn import FlopCountAnalysis, parameter_count\n",
        "\n",
        "    # def get_stats_fvcore(model, input_size=(1, 3, 32, 32)):\n",
        "    #     model = model.eval().cpu()\n",
        "    #     inputs = torch.randn(*input_size)\n",
        "    #     flops = FlopCountAnalysis(model, inputs)\n",
        "    #     params = parameter_count(model)\n",
        "\n",
        "    #     return round(params[\"\"], 2), round(flops.total() / 1e9, 2)\n",
        "\n",
        "    # base_params, base_flops = get_stats_fvcore(base_model)\n",
        "    # sc_params, sc_flops = get_stats_fvcore(scconv_model)\n",
        "\n",
        "    def get_stats_fvcore_from_scratch(use_scconv=False, input_size=(1, 3, 32, 32)):\n",
        "\n",
        "        if use_scconv:\n",
        "            model = ResNetSC([3, 4, 6, 3], num_classes=100)\n",
        "        else:\n",
        "            model = models.resnet50(num_classes=10)\n",
        "\n",
        "        model = model.eval().cpu()\n",
        "        inputs = torch.randn(*input_size)\n",
        "        flops = FlopCountAnalysis(model, inputs)\n",
        "        params = parameter_count(model)\n",
        "\n",
        "        return round(params[\"\"], 2), round(flops.total() / 1e9, 2)\n",
        "\n",
        "    base_params, base_flops = get_stats_fvcore_from_scratch(use_scconv=False)\n",
        "    sc_params, sc_flops = get_stats_fvcore_from_scratch(use_scconv=True)\n",
        "\n",
        "    rows = [\n",
        "        [\"ResNet-50 (Baseline)\", f\"{base_acc:.2f}\", f\"{(end_base-start_base)/60:.2f}\", f\"{base_params}\", f\"{base_flops}\"],\n",
        "        [\"SCConv-ResNet-50\", f\"{scconv_acc:.2f}\", f\"{(end_sc-start_sc)/60:.2f}\", f\"{sc_params}\", f\"{sc_flops}\"],\n",
        "    ]\n",
        "\n",
        "    print(\"\\nSummary:\\n\")\n",
        "    print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))\n",
        "    print(\"\\nComparative Analysis:\")\n",
        "    print(f\"Accuracy Gain:      {acc_gain:+.2f}%\")\n",
        "    print(f\"Time Increase:      {time_diff:+.2f} sec ({time_ratio:.2f}× slower)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7uwc5cHBmWo"
      },
      "source": [
        "# Contribution-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** This contribution is tied to the official CovNeXt V2 repository. Therefore, if you need to work with this, you first need to clone that repo. and then work the models, by embedding the SSConv plug and module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXUG_wKpB9IN"
      },
      "source": [
        "### Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSOPVhT6vuTa",
        "outputId": "31df9b65-b286-40fa-ff6a-370d3ed9c8f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon May 12 13:58:36 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk3rNCUABt2P",
        "outputId": "479b11f5-11aa-40cf-ce9c-adcf2960159c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m529.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore) (2.0.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (3.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore) (11.2.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.7->fvcore) (4.13.2)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=98b7de4613adc5b1825978fe859c3c722b0703a88ffe2744345576ba5caf2266\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=dbf058f8f5cfba40dbb827dc382199cde0381b5a81c6f2ac7b19c6c0c218fe11\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, iopath, nvidia-cusolver-cu12, fvcore\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 portalocker-3.1.1 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip3 install timm fvcore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "O1SA6gl5Byd7"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/facebookresearch/ConvNeXt-V2.git\n",
        "\n",
        "# Note: Only once required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_8QE7Orv7Mg",
        "outputId": "d63e0f14-a599-421d-dbd6-af43a664fe86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRfabDahB0sJ",
        "outputId": "fa936e0b-c4e1-40e5-ede6-f5d4f7ce155c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/ann_project/src/ConvNeXt-V2\n"
          ]
        }
      ],
      "source": [
        "# %cd ConvNeXt-V2\n",
        "%cd /content/drive/MyDrive/ann_project/src/ConvNeXt-V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjm8PqkrvuTi",
        "outputId": "1ce09e4d-f930-4fe3-9050-1e7d07c2d983"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torch.profiler import profile, ProfilerActivity\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "\n",
        "from fvcore.nn import FlopCountAnalysis, parameter_count\n",
        "from tabulate import tabulate\n",
        "\n",
        "from models.convnextv2 import ConvNeXtV2\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "F92k7JLCvuTj"
      },
      "outputs": [],
      "source": [
        "# Enabling cuDNN autotuner and TF32\n",
        "torch.backends.cudnn.benchmark = True\n",
        "if hasattr(torch.backends.cuda, 'matmul_precision'):\n",
        "    torch.set_float32_matmul_precision('high')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tgu9XF-OGOpV"
      },
      "outputs": [],
      "source": [
        "# For, commenting-out unecessary code-blocks; however, the full commenting out of the classes are to be done manually, otherwise we get a syntax error.\n",
        "utils_path = 'models/utils.py'\n",
        "\n",
        "with open(utils_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "with open(utils_path, 'w') as f:\n",
        "\n",
        "    for line in lines:\n",
        "\n",
        "        if 'Minkowski' in line:\n",
        "            f.write(f\"# {line}\")  # comment out\n",
        "        else:\n",
        "            f.write(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNWgTglwB7ID"
      },
      "source": [
        "### Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHnv8FhtvuTj"
      },
      "source": [
        "##### For, testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6mT0ejesw8P",
        "outputId": "43a0752b-4849-48ff-ecf7-c75db9db791e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'models.convnextv2' from '/content/drive/MyDrive/ann_project/src/ConvNeXt-V2/models/convnextv2.py'>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# For, testing\n",
        "import importlib\n",
        "import models.convnextv2\n",
        "# import models.scconv\n",
        "# importlib.reload(models.scconv)\n",
        "importlib.reload(models.convnextv2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_i7GqI-CJl7",
        "outputId": "1809c440-fec7-4bc2-b158-2a977b2012ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output: torch.Size([2, 10])\n"
          ]
        }
      ],
      "source": [
        "# For, testing\n",
        "from models.convnextv2 import ConvNeXtV2\n",
        "\n",
        "model = ConvNeXtV2(depths=[1,1,1,1], dims=[32,64,128,256],\n",
        "                   num_classes=10, use_scconv=True)\n",
        "x = torch.randn(2, 3, 32, 32)\n",
        "y = model(x)\n",
        "print(\"Output:\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX4srPGjnsMs",
        "outputId": "7a46ef67-b99d-455a-a371-03b9e5e448c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline dwconv type: <class 'torch.nn.modules.conv.Conv2d'>\n",
            "SCConv dwconv type: <class 'models.convnextv2.SCConv'>\n"
          ]
        }
      ],
      "source": [
        "# For, testing\n",
        "from models.convnextv2 import ConvNeXtV2\n",
        "\n",
        "# Instantiate with and without SCConv\n",
        "m_base = ConvNeXtV2(depths=[1,1,1,1], dims=[32,64,128,256],\n",
        "                    num_classes=10, use_scconv=False)\n",
        "m_sc   = ConvNeXtV2(depths=[1,1,1,1], dims=[32,64,128,256],\n",
        "                    num_classes=10, use_scconv=True)\n",
        "\n",
        "# Grab the very first Block in stage 0\n",
        "blk_base = m_base.stages[0][0]\n",
        "blk_sc   = m_sc.stages[0][0]\n",
        "\n",
        "print(\"Baseline dwconv type:\", type(blk_base.dwconv))\n",
        "# Expect: <class 'torch.nn.modules.conv.Conv2d'>\n",
        "\n",
        "print(\"SCConv dwconv type:\", type(blk_sc.dwconv))\n",
        "# Expect: <class 'models.scconv.SCConv'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ5NZjtQn0Y4",
        "outputId": "c8bc2325-92f5-460c-b7f5-9d3020104fe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SCConv count in baseline: 0\n",
            "SCConv count in patched model: 4\n"
          ]
        }
      ],
      "source": [
        "# For, testing\n",
        "def count_scconv(m):\n",
        "    return sum(1 for module in m.modules() if module.__class__.__name__ == \"SCConv\")\n",
        "\n",
        "print(\"SCConv count in baseline:\", count_scconv(m_base))  # should be 0\n",
        "print(\"SCConv count in patched model:\", count_scconv(m_sc))  # should equal total blocks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFqsT5ULGpCY",
        "outputId": "1ff2a45f-a441-4e5e-da92-7131987659bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done one epoch.\n"
          ]
        }
      ],
      "source": [
        "# For, testing\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# transforms\n",
        "transform = T.Compose([T.RandomCrop(32,4), T.RandomHorizontalFlip(),\n",
        "                       T.ToTensor(), T.Normalize((.5,)*3,(.5,)*3)])\n",
        "\n",
        "trainset = CIFAR10('./data', True, transform, download=True)\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "# use SCConv model\n",
        "model = ConvNeXtV2(depths=[2,2,2,2], dims=[64,128,256,512],\n",
        "                   num_classes=10, use_scconv=True).cuda()\n",
        "opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# one training epoch\n",
        "model.train()\n",
        "for imgs, lbls in trainloader:\n",
        "    imgs, lbls = imgs.cuda(), lbls.cuda()\n",
        "    opt.zero_grad()\n",
        "    logits = model(imgs)\n",
        "    loss = criterion(logits, lbls)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "print(\"Done one epoch.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyb__VxMjakE"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mmc3-4kevuTl"
      },
      "outputs": [],
      "source": [
        "def get_data_loaders(batch_size=256):\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408),\n",
        "                             (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408),\n",
        "                             (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "\n",
        "    trainset = datasets.CIFAR10('./data', True, transform_train, download=True)\n",
        "    testset  = datasets.CIFAR10('./data', False, transform_test, download=True)\n",
        "\n",
        "    loader_args = dict(batch_size=batch_size,\n",
        "                       num_workers=2,\n",
        "                       pin_memory=True,\n",
        "                       persistent_workers=True)\n",
        "\n",
        "    trainloader = DataLoader(trainset, shuffle=True, **loader_args)\n",
        "    testloader  = DataLoader(testset, shuffle=False, **loader_args)\n",
        "\n",
        "    return trainloader, testloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "isgIV1_RjdBM"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(use_scconv=False, epochs=5):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    trainloader, testloader = get_data_loaders(256)\n",
        "\n",
        "    model = ConvNeXtV2(\n",
        "        in_chans=3,\n",
        "        num_classes=10,\n",
        "        depths=[2,2,6,2],\n",
        "        dims=[64,128,256,512],\n",
        "        drop_path_rate=0.1,\n",
        "        use_scconv=use_scconv\n",
        "    ).to(device).to(memory_format=torch.channels_last)\n",
        "    model = torch.compile(model)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
        "                          momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=0.1,\n",
        "        steps_per_epoch=len(trainloader), epochs=epochs\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs = inputs.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with autocast(device_type='cuda'):\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"> Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(trainloader):.4f}\")\n",
        "\n",
        "    # Profiling example on a single batch\n",
        "    # inputs, labels = next(iter(testloader))\n",
        "    # with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
        "    #     model(inputs.to(device).to(memory_format=torch.channels_last))\n",
        "    # print(prof.key_averages().table(sort_by=\"cuda_time_total\"))\n",
        "\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for inputs, labels in testloader:\n",
        "            inputs = inputs.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            with autocast(device_type='cuda'):\n",
        "                preds = model(inputs).argmax(dim=1)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return 100. * correct / total, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "psqvS0pwvuTl"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_model(model, epochs=5):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = model.to(device).to(memory_format=torch.channels_last)\n",
        "    model = torch.compile(model)\n",
        "\n",
        "    trainloader, testloader = get_data_loaders(256)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
        "                          momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=0.1,\n",
        "        steps_per_epoch=len(trainloader), epochs=epochs\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs = inputs.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with autocast(device_type='cuda'):\n",
        "                loss = criterion(model(inputs), labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs = inputs.to(device).to(memory_format=torch.channels_last)\n",
        "            labels = labels.to(device)\n",
        "            with autocast(device_type='cuda'):\n",
        "                preds = model(inputs).argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100. * correct / total, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJHxoVFAjs6b"
      },
      "source": [
        "### Driver Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5BgxuOCjqq9",
        "outputId": "6c64eb57-0c90-460f-83f0-1f22596d3164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating ConvNeXtV2 (Baseline)...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Epoch 1/5, Loss: 1.8602\n",
            "> Epoch 2/5, Loss: 1.6779\n",
            "> Epoch 3/5, Loss: 1.6130\n",
            "> Epoch 4/5, Loss: 1.5772\n",
            "> Epoch 5/5, Loss: 1.5450\n",
            "\n",
            "Evaluating ConvNeXtV2 + SCConv...\n",
            "> Epoch 1/5, Loss: 1.8789\n",
            "> Epoch 2/5, Loss: 1.7020\n",
            "> Epoch 3/5, Loss: 1.6344\n",
            "> Epoch 4/5, Loss: 1.5916\n",
            "> Epoch 5/5, Loss: 1.5447\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsupported operator aten::mean encountered 21 time(s)\n",
            "Unsupported operator aten::sub encountered 8 time(s)\n",
            "Unsupported operator aten::pow encountered 4 time(s)\n",
            "Unsupported operator aten::add encountered 56 time(s)\n",
            "Unsupported operator aten::sqrt encountered 4 time(s)\n",
            "Unsupported operator aten::div encountered 16 time(s)\n",
            "Unsupported operator aten::mul encountered 28 time(s)\n",
            "Unsupported operator aten::gelu encountered 12 time(s)\n",
            "Unsupported operator aten::linalg_vector_norm encountered 12 time(s)\n",
            "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "stages.0.1.drop_path, stages.1.0.drop_path, stages.1.1.drop_path, stages.2.0.drop_path, stages.2.1.drop_path, stages.2.2.drop_path, stages.2.3.drop_path, stages.2.4.drop_path, stages.2.5.drop_path, stages.3.0.drop_path, stages.3.1.drop_path\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsupported operator aten::mean encountered 21 time(s)\n",
            "Unsupported operator aten::sub encountered 8 time(s)\n",
            "Unsupported operator aten::pow encountered 4 time(s)\n",
            "Unsupported operator aten::add encountered 56 time(s)\n",
            "Unsupported operator aten::sqrt encountered 4 time(s)\n",
            "Unsupported operator aten::div encountered 16 time(s)\n",
            "Unsupported operator aten::mul encountered 28 time(s)\n",
            "Unsupported operator aten::gelu encountered 12 time(s)\n",
            "Unsupported operator aten::linalg_vector_norm encountered 12 time(s)\n",
            "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "stages.0.1.drop_path, stages.1.0.drop_path, stages.1.1.drop_path, stages.2.0.drop_path, stages.2.1.drop_path, stages.2.2.drop_path, stages.2.3.drop_path, stages.2.4.drop_path, stages.2.5.drop_path, stages.3.0.drop_path, stages.3.1.drop_path\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summary:\n",
            "\n",
            "╒═══════════════════════╤════════════════╤═══════════════════════╤══════════════╤═════════════╕\n",
            "│ Model                 │   Accuracy (%) │   Training Time (min) │   Params (M) │   FLOPs (G) │\n",
            "╞═══════════════════════╪════════════════╪═══════════════════════╪══════════════╪═════════════╡\n",
            "│ ConvNeXtV2 (Baseline) │          47.46 │                  1.16 │      8554506 │        0.03 │\n",
            "├───────────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ ConvNeXtV2 + SCConv   │          47.2  │                  0.91 │      5543690 │        0.02 │\n",
            "╘═══════════════════════╧════════════════╧═══════════════════════╧══════════════╧═════════════╛\n",
            "\n",
            "Comparative Analysis:\n",
            "Accuracy Gain:      -0.26%\n",
            "Time Increase:      -14.66 sec (0.79× slower)\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    from tabulate import tabulate\n",
        "\n",
        "    print(\"Evaluating ConvNeXtV2 (Baseline)...\")\n",
        "    start_base = time.time()\n",
        "    base_acc, base_model = train_and_evaluate(use_scconv=False)\n",
        "    end_base = time.time()\n",
        "\n",
        "    print(\"\\nEvaluating ConvNeXtV2 + SCConv...\")\n",
        "    start_sc = time.time()\n",
        "    sc_acc, sc_model = train_and_evaluate(use_scconv=True)\n",
        "    end_sc = time.time()\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Training times\n",
        "    base_time = (end_base - start_base)\n",
        "    sc_time   = (end_sc   - start_sc)\n",
        "    base_time_min = base_time / 60\n",
        "    sc_time_min   = sc_time / 60\n",
        "\n",
        "    # Accuracy gain and timing stats\n",
        "    acc_gain  = sc_acc - base_acc\n",
        "    time_diff = sc_time - base_time\n",
        "    time_ratio = sc_time / base_time if base_time != 0 else float('inf')\n",
        "\n",
        "    from fvcore.nn import FlopCountAnalysis, parameter_count\n",
        "    import torch\n",
        "\n",
        "    def stats(model):\n",
        "        model.eval()\n",
        "        if hasattr(model, \"_orig_mod\"):  # for torch.compile() models\n",
        "            model = model._orig_mod\n",
        "        dummy = torch.randn(1, 3, 32, 32).to(next(model.parameters()).device)\n",
        "        with torch.no_grad():\n",
        "            flops = FlopCountAnalysis(model.cpu(), dummy.cpu())\n",
        "            params = parameter_count(model.cpu())\n",
        "        return round(params[\"\"], 2), round(flops.total() / 1e9, 2)\n",
        "\n",
        "    base_params, base_flops = stats(base_model)\n",
        "    sc_params, sc_flops     = stats(sc_model)\n",
        "\n",
        "    rows = [\n",
        "        [\"ConvNeXtV2 (Baseline)\", f\"{base_acc:.2f}\", f\"{base_time_min:.2f}\", base_params, base_flops],\n",
        "        [\"ConvNeXtV2 + SCConv\",   f\"{sc_acc:.2f}\",   f\"{sc_time_min:.2f}\",   sc_params,   sc_flops],\n",
        "    ]\n",
        "\n",
        "    headers = [\"Model\", \"Accuracy (%)\", \"Training Time (min)\", \"Params (M)\", \"FLOPs (G)\"]\n",
        "\n",
        "    print(\"\\nSummary:\\n\")\n",
        "    print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))\n",
        "\n",
        "    print(\"\\nComparative Analysis:\")\n",
        "    print(f\"Accuracy Gain:      {acc_gain:+.2f}%\")\n",
        "    print(f\"Time Increase:      {time_diff:+.2f} sec ({time_ratio:.2f}× slower)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ5EMF_hvuTm",
        "outputId": "d595c08a-a152-45b6-efcf-03b462a262a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating ConvNeXtV2-atto (Baseline)...\n",
            "\n",
            "Evaluating ConvNeXtV2-atto + SCConv...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 21 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pow encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 56 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sqrt encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 16 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 28 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 12 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::linalg_vector_norm encountered 12 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 21 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pow encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 56 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sqrt encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 16 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 28 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 12 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::linalg_vector_norm encountered 12 time(s)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating ConvNeXtV2-femto (Baseline)...\n",
            "\n",
            "Evaluating ConvNeXtV2-femto + SCConv...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0512 15:16:01.330000 1003 torch/_dynamo/convert_frame.py:906] [0/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "W0512 15:16:01.330000 1003 torch/_dynamo/convert_frame.py:906] [0/8]    function: 'forward' (/content/drive/MyDrive/ann_project/src/ConvNeXt-V2/models/convnextv2.py:547)\n",
            "W0512 15:16:01.330000 1003 torch/_dynamo/convert_frame.py:906] [0/8]    last reason: 0/0: GLOBAL_STATE changed: grad_mode \n",
            "W0512 15:16:01.330000 1003 torch/_dynamo/convert_frame.py:906] [0/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "W0512 15:16:01.330000 1003 torch/_dynamo/convert_frame.py:906] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 21 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pow encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 56 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sqrt encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 16 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 28 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 12 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::linalg_vector_norm encountered 12 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 21 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pow encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 56 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sqrt encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 16 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 28 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 12 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::linalg_vector_norm encountered 12 time(s)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating ConvNeXtV2-pico (Baseline)...\n",
            "\n",
            "Evaluating ConvNeXtV2-pico + SCConv...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 21 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pow encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 56 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sqrt encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 16 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 28 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 12 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::linalg_vector_norm encountered 12 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 21 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pow encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 56 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sqrt encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 16 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 28 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 12 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::linalg_vector_norm encountered 12 time(s)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating ConvNeXtV2-nano (Baseline)...\n",
            "\n",
            "Evaluating ConvNeXtV2-nano + SCConv...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 23 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pow encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 64 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sqrt encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 18 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 32 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 14 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::linalg_vector_norm encountered 14 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 23 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pow encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 64 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sqrt encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 18 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 32 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 14 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::linalg_vector_norm encountered 14 time(s)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating ConvNeXtV2-tiny (Baseline)...\n",
            "\n",
            "Evaluating ConvNeXtV2-tiny + SCConv...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 27 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pow encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 80 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sqrt encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 22 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 40 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 18 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::linalg_vector_norm encountered 18 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 27 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pow encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 80 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sqrt encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 22 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 40 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 18 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::linalg_vector_norm encountered 18 time(s)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating ConvNeXtV2-base (Baseline)...\n",
            "\n",
            "Evaluating ConvNeXtV2-base + SCConv...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 45 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pow encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 152 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sqrt encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 40 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 76 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 36 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::linalg_vector_norm encountered 36 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 45 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pow encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 152 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sqrt encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 40 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 76 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 36 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::linalg_vector_norm encountered 36 time(s)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating ConvNeXtV2-large (Baseline)...\n",
            "\n",
            "Evaluating ConvNeXtV2-large + SCConv...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 45 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pow encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 152 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sqrt encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 40 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 76 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 36 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::linalg_vector_norm encountered 36 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 45 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pow encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 152 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sqrt encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 40 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 76 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 36 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::linalg_vector_norm encountered 36 time(s)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summary:\n",
            "\n",
            "╒══════════════════╤════════════════╤═══════════════════════╤══════════════╤═════════════╕\n",
            "│ Model            │   Accuracy (%) │   Training Time (min) │   Params (M) │   FLOPs (G) │\n",
            "╞══════════════════╪════════════════╪═══════════════════════╪══════════════╪═════════════╡\n",
            "│ atto (Baseline)  │          47.58 │                  2.05 │      3388170 │        0.01 │\n",
            "├──────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ atto + SCConv    │          45.65 │                  2.16 │      2180330 │        0.01 │\n",
            "├──────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ femto (Baseline) │          47.13 │                  2.61 │      4849162 │        0.02 │\n",
            "├──────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ femto + SCConv   │          46.9  │                  3.37 │      3130186 │        0.01 │\n",
            "├──────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ pico (Baseline)  │          48    │                  2.4  │      8554506 │        0.03 │\n",
            "├──────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ pico + SCConv    │          48.38 │                  2.41 │      5543690 │        0.02 │\n",
            "├──────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ nano (Baseline)  │          48.58 │                  2.48 │     14983690 │        0.05 │\n",
            "├──────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ nano + SCConv    │          46.75 │                  2.51 │      9677770 │        0.03 │\n",
            "├──────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ tiny (Baseline)  │          48.74 │                  2.63 │     27866122 │        0.09 │\n",
            "├──────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ tiny + SCConv    │          49.81 │                  2.66 │     17856970 │        0.06 │\n",
            "├──────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ base (Baseline)  │          49.75 │                  3.48 │     87683082 │        0.31 │\n",
            "├──────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ base + SCConv    │          50.34 │                  3.53 │     55444746 │        0.2  │\n",
            "├──────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ large (Baseline) │          48.93 │                  4.87 │    196405258 │        0.7  │\n",
            "├──────────────────┼────────────────┼───────────────────────┼──────────────┼─────────────┤\n",
            "│ large + SCConv   │          49.57 │                  3.72 │    124491658 │        0.44 │\n",
            "╘══════════════════╧════════════════╧═══════════════════════╧══════════════╧═════════════╛\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    from tabulate import tabulate\n",
        "\n",
        "    from models.convnextv2 import convnextv2_atto, convnextv2_femto, convnext_pico, convnextv2_nano, convnextv2_tiny, convnextv2_base, convnextv2_large\n",
        "\n",
        "    model_variants = {\n",
        "        \"atto\": convnextv2_atto,\n",
        "        \"femto\":convnextv2_femto,\n",
        "        \"pico\": convnext_pico,\n",
        "        \"nano\": convnextv2_nano,\n",
        "        \"tiny\": convnextv2_tiny,\n",
        "        \"base\": convnextv2_base,\n",
        "        \"large\":convnextv2_large\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "\n",
        "    def stats(model):\n",
        "        model.eval()\n",
        "        if hasattr(model, \"_orig_mod\"):  # for torch.compile() models\n",
        "            model = model._orig_mod\n",
        "        dummy = torch.randn(1, 3, 32, 32).to(next(model.parameters()).device)\n",
        "        with torch.no_grad():\n",
        "            flops = FlopCountAnalysis(model.cpu(), dummy.cpu())\n",
        "            params = parameter_count(model.cpu())\n",
        "        return round(params[\"\"], 2), round(flops.total() / 1e9, 2)\n",
        "\n",
        "    for name, model_fn in model_variants.items():\n",
        "        print(f\"\\nEvaluating ConvNeXtV2-{name} (Baseline)...\")\n",
        "        start_base = time.time()\n",
        "        base_model = model_fn(in_chans=3, num_classes=10, use_scconv=False)\n",
        "        base_acc, _ = train_and_evaluate_model(base_model)\n",
        "        end_base = time.time()\n",
        "\n",
        "        print(f\"\\nEvaluating ConvNeXtV2-{name} + SCConv...\")\n",
        "        start_sc = time.time()\n",
        "        sc_model = model_fn(in_chans=3, num_classes=10, use_scconv=True)\n",
        "        sc_acc, _ = train_and_evaluate_model(sc_model)\n",
        "        end_sc = time.time()\n",
        "\n",
        "        base_time = end_base - start_base\n",
        "        sc_time = end_sc - start_sc\n",
        "\n",
        "        base_params, base_flops = stats(base_model)\n",
        "        sc_params, sc_flops     = stats(sc_model)\n",
        "\n",
        "        results.append([\n",
        "            f\"{name} (Baseline)\", f\"{base_acc:.2f}\", f\"{base_time/60:.2f}\", base_params, base_flops\n",
        "        ])\n",
        "        results.append([\n",
        "            f\"{name} + SCConv\", f\"{sc_acc:.2f}\", f\"{sc_time/60:.2f}\", sc_params, sc_flops\n",
        "        ])\n",
        "\n",
        "    headers = [\"Model\", \"Accuracy (%)\", \"Training Time (min)\", \"Params (M)\", \"FLOPs (G)\"]\n",
        "    print(\"\\nSummary:\\n\")\n",
        "    print(tabulate(results, headers=headers, tablefmt=\"fancy_grid\"))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
